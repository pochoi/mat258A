{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "Chi Po Choi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Methods code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtminBFGS (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified Newton method\n",
    "# Modify the eigenvalues to be positive\n",
    "function BkFunInv(H, ε)\n",
    "    D, V = eig(H)\n",
    "    Dp = ifelse(D .> ε, D, ε)\n",
    "    Dpinv = 1./Dp\n",
    "    return V * Diagonal(Dpinv) * V'\n",
    "end\n",
    "\n",
    "# Back tracking method\n",
    "function backTracking(obj, x, d, g)\n",
    "    α = 1\n",
    "    while (obj(x + α*d)[1] > obj(x)[1] + α * 1e-4 * dot(g, d))\n",
    "       α = α * 0.5\n",
    "       if α < 1e-6\n",
    "           break\n",
    "       end\n",
    "    end\n",
    "    return α\n",
    "end\n",
    "\n",
    "function wolfe(obj, x, d, g)\n",
    "\tt = 1\n",
    "\talpha = 0\n",
    "\tbeta = Inf\n",
    "\tc1 = 1e-4\n",
    "\tc2 = 0.9\n",
    "\tmaxItr = 50\n",
    "\tfor i = 1:maxItr\n",
    "\t\tif (obj(x + t*d)[1] > obj(x)[1] + t * c1 * dot(g, d))\n",
    "\t\t\tbeta = t\n",
    "\t\t\tt = (alpha + beta)/2\n",
    "\t\telseif dot(obj(x + t*d)[2], d) < c2 * dot(g, d)\n",
    "\t\t\talpha = t\n",
    "\t\t\tif isinf(beta)\n",
    "\t\t\t\tt = 2*alpha\n",
    "\t\t\telse\n",
    "\t\t\t\tt = (alpha + beta)/2\n",
    "\t\t\tend\n",
    "\t\telse\n",
    "\t\t\tbreak\n",
    "\t\tend\n",
    "\tend\n",
    "\treturn t\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# Newton Method\n",
    "function newtmin(obj, x0; maxIts=1000, optTol=1e-6, BkFlag = true, btFlag = true)\n",
    "    # Minimize a function f using Newton’s method.\n",
    "    # obj: a function that evaluates the objective value,\n",
    "    # gradient, and Hessian at a point x, i.e.,\n",
    "    # (f, g, H) = obj(x)\n",
    "    # x0: starting point.\n",
    "    # maxIts (optional): maximum number of iterations.\n",
    "    # optTol (optional): optimality tolerance based on\n",
    "    # ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    # BkFlag (optional): true for doing Modified Hessian\n",
    "    # btFlag (optional): true for doing Back Tracking\n",
    "    f0, g0, H0 = obj(x0)\n",
    "    its = 0\n",
    "    Opt = Float64[]\n",
    "    xkp = x0\n",
    "    for i in 1:maxIts\n",
    "        xk = xkp\n",
    "        fk, gk, Hk = obj(xk)\n",
    "        opt = norm(gk, 2)\n",
    "        push!(Opt, opt)\n",
    "        #if opt < optTol*norm(g0)\n",
    "        if opt < optTol\n",
    "            break\n",
    "        end\n",
    "        if BkFlag == true\n",
    "            Bkinv = BkFunInv(Hk, 0.01)\n",
    "            dk = Bkinv * (- gk)\n",
    "        else\n",
    "            dk = Hk \\ (- gk)\n",
    "        end\n",
    "        if btFlag == true\n",
    "            αk = backTracking(obj, xk, dk, gk)\n",
    "            #αk = wolfe(obj, xk, dk, gk)\n",
    "        else\n",
    "            αk = 1\n",
    "        end\n",
    "        xkp = xk + αk * dk\n",
    "        its = its + 1\n",
    "    end\n",
    "    return xkp, its, Opt\n",
    "end\n",
    "\n",
    "# BFGS\n",
    "function newtminBFGS(obj, x0; maxIts=1000, optTol=1e-6, btFlag=true)\n",
    "    # Minimize a function f using BFGS Newton’s method.\n",
    "    # obj: a function that evaluates the objective value,\n",
    "    # gradient, and Hessian at a point x, i.e.,\n",
    "    # (f, g, H) = obj(x)\n",
    "    # x0: starting point.\n",
    "    # maxIts (optional): maximum number of iterations.\n",
    "    # optTol (optional): optimality tolerance based on\n",
    "    # ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    # btFlag (optional): true for doing Back Tracking\n",
    "    f0, g0, H0 = obj(x0)\n",
    "    #D, V = eig(H0)\n",
    "    #Hkp = V * Diagonal(map(x -> 1/max(x,1), D)) * V'\n",
    "    Hkp = H0\n",
    "    its = 0\n",
    "    Opt = Float64[]\n",
    "    xkp = copy(x0)\n",
    "    for i in 1:maxIts\n",
    "        xk = copy(xkp)\n",
    "        Hk = copy(Hkp)\n",
    "        gk = obj(xk)[2]\n",
    "        opt = norm(gk, 2)\n",
    "        push!(Opt, opt)\n",
    "        #if opt < optTol*norm(g0)\n",
    "        if opt < optTol\n",
    "            break\n",
    "        end\n",
    "        dk = Hk * (- gk)\n",
    "        if btFlag == true\n",
    "            #αk = backTracking(obj, xk, dk, gk)\n",
    "            αk = wolfe(obj, xk, dk, gk)\n",
    "        else\n",
    "            αk = 1\n",
    "        end\n",
    "        xkp = xk + αk * dk\n",
    "        fkp, gkp, Hkp = obj(xkp)\n",
    "        sk = xkp - xk\n",
    "        yk = gkp - gk\n",
    "        ρk = 1/(yk' * sk)[]\n",
    "        Hkp = (eye(Hk) - ρk * sk * yk') * Hk * (eye(Hk) - ρk * yk * sk') + ρk * sk * sk'\n",
    "        its = its + 1\n",
    "    end\n",
    "    return xkp, its\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The test problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prob (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function prob(k)\n",
    "\tif k == 6\n",
    "\t\tx0 = [-1.2, 1]\n",
    "\t\tf(x::Vector) = (1-x[1])^2\n",
    "\t\tc(x::Vector) = [ 10*(x[2] - x[1])^2 ]\n",
    "\telseif k == 7\n",
    "\t\tx0 = [2.0, 2.0]\n",
    "\t\tf(x::Vector) = log(1+x[1]^2) - x[2]\n",
    "\t\tc(x::Vector) = [(1 + x[1]^2)^2 + x[2]^2 - 4 ]\n",
    "\telseif k == 8\n",
    "\t\tx0 = [2.0, 1.0]\n",
    "\t\tf(x::Vector) = -1\n",
    "\t\tc(x::Vector) = [ x[1]^2 + x[2]^2 - 25 , x[1]*x[2] - 9 ]\n",
    "\telseif k == 9\n",
    "\t\tx0 = [0.0, 0.0]\n",
    "\t\tf(x::Vector) = sin(pi*x[1]/12) * cos(pi*x[2]/16)\n",
    "\t\tc(x::Vector) = [4*x[1] - 3*x[2]]\n",
    "\telse\n",
    "\t\tx0 = [123.0, 321.0]\n",
    "\t\tf(x::Vector) = 100\n",
    "\t\tc(x::Vector) = [ x[1] - 123, x[2] - 321 ]\n",
    "\tend\n",
    "\n",
    "\treturn (x0, f, c)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Lagrangian Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AugLag (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ForwardDiff\n",
    "\n",
    "function AugLag(myprob; newtonFlag = true, maxItr=30, optTolck = 1e-8, optTolKKT = 1e-8)\n",
    "\n",
    "\tx0 = myprob[1]\n",
    "\trho0 = 10.0\n",
    "\ty0 = zeros(myprob[3](myprob[1]))\n",
    "\t\n",
    "\trhok = rho0\n",
    "\tyk = y0\n",
    "\txk = copy(x0)\n",
    "\n",
    "\tfunction obj(x0, myprob, y, rho)\n",
    "\t\tf(x) = myprob[2](x) - dot(y, myprob[3](x)) + rho / 2 * dot(myprob[3](x), myprob[3](x))\n",
    "\t\tg = ForwardDiff.gradient(f)\n",
    "\t\th = ForwardDiff.jacobian(g)\n",
    "        return (f(x0), g(x0), h(x0))\n",
    "    end\n",
    "\n",
    "    g0 = ForwardDiff.gradient(myprob[2])\n",
    "    j = ForwardDiff.jacobian(myprob[3])\n",
    "\n",
    "    rho_counter = 0\n",
    "\n",
    "\tcounter = 0    \n",
    "\n",
    "\tfor itr = 1:maxItr\n",
    "\t\tif newtonFlag == true\n",
    "\t\t\txkp = newtmin(x -> obj(x, myprob, yk, rhok), xk, maxIts=1000, optTol = (1e-6 / 10^itr ) )[1]\n",
    "\t\telse \n",
    "\t\t\txkp = newtminBFGS(x -> obj(x, myprob, yk, rhok), xk, maxIts=1000, optTol = (1e-6 / 10^itr ), btFlag=true)[1]\n",
    "\t\tend\n",
    "\n",
    "\t\tif norm(myprob[3](xkp)) < 1e-4 * (1/2)^rho_counter\n",
    "\t\t\trhokp = rhok\n",
    "\t\t\tykp = yk - rhokp * myprob[3](xkp)\n",
    "\t\t\trho_counter = rho_counter + 1\n",
    "\t\telse\n",
    "\t\t\trhokp = 10 * rhok\n",
    "\t\t\tykp = yk\n",
    "\t\tend\n",
    "\n",
    "\t\trhok = rhokp\n",
    "\t\tyk = ykp\n",
    "\t\txk = copy(xkp)\n",
    "\n",
    "\t\tcounter = itr\n",
    "\t\tif (norm(myprob[3](xk)) < optTolck) & (norm( g0(xk) - j(xk)' * yk) < optTolKKT)\n",
    "\t\t\tbreak\n",
    "\t\tend\n",
    "\n",
    "\tend\n",
    "\n",
    "\treturn xk, yk, rhok, counter\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Augmented Lagrangian Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the performance between Newton method and quasi-Newton method BFGS\n",
      "  Prob   Newton Itr     BFGS Itr\n",
      "No.  6            3            2\n",
      "No.  7            5            5\n",
      "No.  8            1            1\n",
      "No.  9            4            4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xk = Array{Float64,1}[]\n",
    "xkBFGS = Array{Float64,1}[]\n",
    "\n",
    "yk = Array{Float64,1}[]\n",
    "ykBFGS = Array{Float64,1}[]\n",
    "\n",
    "rhok = Float64[]\n",
    "rhokBFGS = Float64[]\n",
    "\n",
    "counter = Int64[]\n",
    "counterBFGS = Int64[]\n",
    "\n",
    "\n",
    "for k = 1:4\n",
    "\t(x, y, rho, cnt) = AugLag(prob(k+5))\n",
    "\t(xb, yb, rhob, cntb)  = AugLag(prob(k+5), newtonFlag = false)\n",
    "\n",
    "\tpush!(xk, x)\n",
    "\tpush!(yk, y)\n",
    "\tpush!(rhok, rho)\n",
    "\tpush!(counter, cnt)\n",
    "\n",
    "\tpush!(xkBFGS, xb)\n",
    "\tpush!(ykBFGS, yb)\n",
    "\tpush!(rhokBFGS, rhob)\n",
    "\tpush!(counterBFGS, cntb)\n",
    "end\n",
    "\n",
    "@printf(\"Comparing the performance between Newton method and quasi-Newton method BFGS\\n\")\n",
    "@printf(\"%6s %12s %12s\\n\", \"Prob\", \"Newton Itr\", \"BFGS Itr\")\n",
    "for k = 1:4\n",
    "    @printf(\"No.%3d %12d %12d\\n\", k+5, counter[k], counterBFGS[k])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, Newton method and BFGS perform similarly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
