{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Toms566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare some useful subfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backTracking (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the Toms566.Problem Type to the objective function format\n",
    "function p2obj(p::Toms566.Problem)\n",
    "    function obj(x)\n",
    "        return (p.obj(x), p.grd(x), p.hes(x))\n",
    "    end\n",
    "    return obj\n",
    "end\n",
    "\n",
    "# Modified Newton method\n",
    "# Modify the eigenvalues to be positive\n",
    "function BkFunInv(H, ε)\n",
    "    D, V = eig(H)\n",
    "    Dp = ifelse(D .> ε, D, ε)\n",
    "    Dpinv = 1./Dp\n",
    "    return V * Diagonal(Dpinv) * V'\n",
    "end\n",
    "\n",
    "# Back tracking method\n",
    "function backTracking(obj, x, d, g)\n",
    "    α = 1\n",
    "    while (obj(x + α*d)[1] > obj(x)[1] + α * 1e-4 * dot(g, d) || \n",
    "        dot(d, obj(x + α*d)[2]) < 0.9 * dot(d, g))\n",
    "       α = α * 0.5 \n",
    "       if α < 1e-7\n",
    "           break\n",
    "       end\n",
    "    end\n",
    "    return α\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtmin (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Newton Method\n",
    "function newtmin(obj, x0; maxIts=1000, optTol=1e-6, BkFlag = true, btFlag = true)\n",
    "    # Minimize a function f using Newton’s method.\n",
    "    # obj: a function that evaluates the objective value,\n",
    "    # gradient, and Hessian at a point x, i.e.,\n",
    "    # (f, g, H) = obj(x)\n",
    "    # x0: starting point.\n",
    "    # maxIts (optional): maximum number of iterations.\n",
    "    # optTol (optional): optimality tolerance based on\n",
    "    # ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    # BkFlag (optional): true for doing Modified Hessian\n",
    "    # btFlag (optional): true for doing Back Tracking\n",
    "    f0, g0, H0 = obj(x0)\n",
    "    its = 0\n",
    "    xkp = x0\n",
    "    for i in 1:maxIts\n",
    "        xk = xkp\n",
    "        fk, gk, Hk = obj(xk)\n",
    "        if norm(gk, 2) < optTol*norm(g0)\n",
    "            break\n",
    "        end\n",
    "        if BkFlag == true\n",
    "            Bkinv = BkFunInv(Hk, 0.01)\n",
    "            dk = Bkinv * (- gk)\n",
    "        else\n",
    "            dk = Hk \\ (- gk)\n",
    "        end\n",
    "        if btFlag == true\n",
    "            αk = backTracking(obj, xk, dk, gk)\n",
    "        else\n",
    "            αk = 1\n",
    "        end\n",
    "        xkp = xk + αk * dk\n",
    "        its = its + 1\n",
    "    end\n",
    "    return xkp, its\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The BFGS Newton’s method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtminBFGS (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtminBFGS(obj, x0; maxIts=1000, optTol=1e-6, btFlag=true)\n",
    "    # Minimize a function f using BFGS Newton’s method.\n",
    "    # obj: a function that evaluates the objective value,\n",
    "    # gradient, and Hessian at a point x, i.e.,\n",
    "    # (f, g, H) = obj(x)\n",
    "    # x0: starting point.\n",
    "    # maxIts (optional): maximum number of iterations.\n",
    "    # optTol (optional): optimality tolerance based on\n",
    "    # ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    # btFlag (optional): true for doing Back Tracking\n",
    "    f0, g0, H0 = obj(x0)\n",
    "    D, V = eig(H0)\n",
    "    Hkp = V * Diagonal(map(x -> 1/max(x,1), D)) * V'\n",
    "    its = 0\n",
    "    xkp = copy(x0)\n",
    "    for i in 1:maxIts\n",
    "        xk = copy(xkp)\n",
    "        Hk = copy(Hkp)\n",
    "        gk = obj(xk)[2]\n",
    "        if norm(gk, 2) < optTol*norm(g0)\n",
    "            break\n",
    "        end\n",
    "        dk = Hk * (- gk)\n",
    "        if btFlag == true\n",
    "            αk = backTracking(obj, xk, dk, gk)\n",
    "        else\n",
    "            αk = 1\n",
    "        end\n",
    "        xkp = xk + αk * dk\n",
    "        fkp, gkp, Hkp = obj(xkp)\n",
    "        sk = xkp - xk\n",
    "        yk = gkp - gk\n",
    "        ρk = 1/(yk' * sk)[]\n",
    "        Hkp = (eye(Hk) - ρk * sk * yk') * Hk * (eye(Hk) - ρk * yk * sk') + ρk * sk * sk'\n",
    "        its = its + 1\n",
    "    end\n",
    "    return xkp, its\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple test function\n",
    "\n",
    "$$\n",
    "f(x) = (x_1 - 10)^2 + (x_2 - 20)^4\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10.0,20.036087439178605],19)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple test function\n",
    "#\n",
    "function simple_hello(x)\n",
    "    f = (x[1] - 10)^2 + (x[2] - 20)^4\n",
    "    g = [ 2*(x[1] - 10) , 4*(x[2] - 20)^3]\n",
    "    H = [[2,0] [0, 4*3*(x[2] - 20)^2]]\n",
    "    return f, g, H\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the simple test function\n",
    "The solution should be $(x_1 = 10, x_2 = 20)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10.0,20.036087439178605],19)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtmin(simple_hello, [100,100], optTol=1e-10, btFlag=false, BkFlag=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton Method gives $(x_1 = 10.0, x_2 = 20.036)$.\n",
    "It requires 19 steps to the tolerace 10e-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Newton methods with Toms566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_test (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function final_test()\n",
    "\n",
    "    btFlag_v = Bool[0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    BkFlag_v = Bool[0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "    @printf(\"%3s %12s %12s %10s %10s %12s %12s %4s\\n\",\n",
    "    \"No.\", \"f(x0)\",\"|∇f(x0)|\", \"BackTrack\" , \"ModNewt\" ,\"f(x*)\",\"|∇f(x*)|\", \"Its\")\n",
    "    for i = 1:18\n",
    "    p = Problem(i)\n",
    "    pans = newtmin(p2obj(p), p.x0, maxIts = 2000, btFlag=btFlag_v[i], BkFlag=BkFlag_v[i])\n",
    "    @printf(\"%3i %12.2e %12.2e %10s %10s %12.2e %12.2e %4i\\n\",\n",
    "                i, p.obj(p.x0), norm(p.grd(p.x0)), \n",
    "                btFlag_v[i], BkFlag_v[i],\n",
    "                p.obj(pans[1]), norm(p.grd(pans[1])), pans[2]\n",
    "    )\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.        f(x0)     |∇f(x0)|  BackTrack    ModNewt        f(x*)     |∇f(x*)|  Its\n",
      "  1     2.50e+03     1.88e+03      false      false     4.10e-12     1.88e-05   32\n",
      "  2     7.79e-01     2.55e+00       true       true     2.61e-01     1.30e-01 2000\n",
      "  3     3.89e-06     7.45e-03      false      false     1.13e-08     9.70e-11    2\n",
      "  4     1.14e+00     2.00e+04       true       true     2.37e-04     1.34e+01 2000\n",
      "  5     1.03e+03     1.49e+02      false      false     4.95e-08     1.30e-04    7\n",
      "  6     9.39e+10     1.01e+11      false      false     5.08e+03     6.78e+04   17\n",
      "  7     3.00e+01     1.78e+02       true       true     5.44e+00     1.79e+01 2000\n",
      "  8     5.45e+09     8.02e+07      false      false     4.78e+01     7.40e+01   11\n",
      "  9     2.87e+05     3.28e+05      false      false     8.81e+01     3.05e-01  228\n",
      " 10     1.00e+12     2.00e+06       true       true     7.06e-25     1.68e-06    8\n",
      " 11     7.93e+06     2.14e+06       true       true     1.60e+06     4.85e+05 2000\n",
      " 12     1.21e+01     3.97e+01       true       true     6.20e+00     1.25e+01 2000\n",
      " 13     2.01e-03     5.30e-02       true       true     7.48e-08     8.95e-08 2000\n",
      " 14     4.84e+02     1.04e+03       true       true     3.54e+02     8.44e+02 2000\n",
      " 15     3.23e+03     1.78e+03       true       true     1.43e+03     9.76e+02 2000\n",
      " 16     1.42e+01     2.78e+01       true       true     4.78e-11     1.71e-05    4\n",
      " 17     1.92e+04     1.64e+04       true       true     1.24e-09     1.48e-03   37\n",
      " 18     1.39e-02     2.65e+00       true       true     8.25e-03     6.83e-01 2000\n"
     ]
    }
   ],
   "source": [
    "final_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of the Newton methods\n",
    "\n",
    "For problem 1, 3, 5, 6, 8, 9, 10, 16 and 17,\n",
    "the algorithm converges (before the maxIts) within the optional tolerace.\n",
    "\n",
    "For problem 1, 3, 5, 6, 8, and 9, only the most simple Newton's method is used, that is, with $\\alpha = 1$ and no modified Hessian. For the remaining problems, $\\alpha$ is searched with Backtracking with Wolfe's condition, and also the Hessian is modified so that all eigenvalues are positive.\n",
    "\n",
    "For those problems reaching maxIts, some of the optimal values still look good (i.e. very small compared to initial values), for example, problem 4, 13 and 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BFGS methods with Toms566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BFGS_test (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function BFGS_test()\n",
    "    @printf(\"%3s  %12s %12s %12s %12s %4s\\n\",\n",
    "    \"No.\", \"f(x0)\",\"|∇f(x0)|\", \"f(x*)\",\"|∇f(x*)|\", \"Its\")\n",
    "    for i = 1:18\n",
    "    p = Problem(i)\n",
    "    pans = newtminBFGS(p2obj(p), p.x0,  maxIts = 2000, btFlag=true)\n",
    "    @printf(\"%3i %12.2e %12.2e %12.2e %12.2e %4i\\n\",\n",
    "                i, p.obj(p.x0), norm(p.grd(p.x0)),\n",
    "                p.obj(pans[1]), norm(p.grd(pans[1])), pans[2]\n",
    "    )\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.         f(x0)     |∇f(x0)|        f(x*)     |∇f(x*)|  Its\n",
      "  1     2.50e+03     1.88e+03     7.86e-10     1.05e-03   25\n",
      "  2     7.79e-01     2.55e+00     1.94e-13     4.57e-07  106\n",
      "  3     3.89e-06     7.45e-03     1.13e-08     6.98e-11    4\n",
      "  4     1.14e+00     2.00e+04     1.19e-11     6.81e-03  144\n",
      "  5     1.03e+03     1.49e+02     4.62e-03     8.78e-03 2000\n",
      "  6     9.39e+10     1.01e+11     1.38e+03     1.00e+05   19\n",
      "  7     3.00e+01     1.78e+02     6.21e-06     1.89e-05   70\n",
      "  8     5.45e+09     8.02e+07     4.47e+01     7.04e+01   25\n",
      "  9     2.87e+05     3.28e+05     8.81e+01     5.51e-01 2000\n",
      " 10     1.00e+12     2.00e+06     1.27e-13     7.10e-01   26\n",
      " 11     7.93e+06     2.14e+06     8.58e+04     8.53e-01   19\n",
      " 12     1.21e+01     3.97e+01     5.19e-13     4.92e-07   32\n",
      " 13     2.01e-03     5.30e-02     3.95e-06     5.22e-08   53\n",
      " 14     4.84e+02     1.04e+03     8.71e-08     9.55e-04  218\n",
      " 15     3.23e+03     1.78e+03     4.57e-06     9.33e-04  172\n",
      " 16     1.42e+01     2.78e+01     2.22e-13     2.31e-06   12\n",
      " 17     1.92e+04     1.64e+04     7.87e+00     1.42e+00 2000\n",
      " 18     1.39e-02     2.65e+00     5.39e-03     2.03e-06  334\n"
     ]
    }
   ],
   "source": [
    "BFGS_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of BFGS\n",
    "\n",
    "We can see that BFGS converges (stop before maxIts) in most of the problems: 1, 2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16 and 18. For those problems reaching maxIts, the optimal values are still small compared to initial values. Therefore, BFGS gives very good results, compared to the Newton methods previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1x400 Array{Float64,2}:\n",
       " 0.95  1.65  2.0  1.6  1.3  1.9  1.4  …  1.15  1.55  1.4  1.15  1.75  1.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data set\n",
    "(rawdata, head) = readdlm(\"binary.csv\", ',', header=true)\n",
    "y = rawdata[:,1]\n",
    "u = [ones(400,1) rawdata[:,2:3]]'\n",
    "\n",
    "# Scale the data for steepest descent\n",
    "us = copy(u)\n",
    "us[2,:] = us[2,:] /400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logRegObj (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Objective function of the log likelihood\n",
    "# It is used in the Newton methods algorithms\n",
    "\n",
    "function logRegObj(x)\n",
    "    fi = [ -(y[i] * (x'*u[:,i]) - log(1+exp(x'*u[:,i]))) for i = 1:400]\n",
    "    f = sum(fi)[]\n",
    "    wi = Float64[ -(y[i] - exp(x'*u[:,i])/(1+exp(x'*u[:,i])))[] for i = 1:400]\n",
    "    g = zeros(3)\n",
    "    for i = 1:400\n",
    "        g = g + wi[i] * u[:,i]\n",
    "    end\n",
    "    h = zeros(3,3)\n",
    "    for i = 1:400 \n",
    "        h = h + (exp(dot(x,u[:,i]))/(1+exp(dot(x,u[:,i])))^2) * (u[:,i] * u[:,i]')\n",
    "    end\n",
    "    return f,g,h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradDescent (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steepest Descent Algorithm\n",
    "# Same as last homework\n",
    "\n",
    "function f(x) \n",
    "    ansi = [ -(y[i] * (x'*us[:,i]) - log(1+exp(x'*us[:,i]))) for i = 1:400]\n",
    "    return sum(ansi)\n",
    "end\n",
    "\n",
    "function grad(x)\n",
    "    wi = Float64[ -(y[i] - exp(x'*us[:,i])/(1+exp(x'*us[:,i])))[] for i = 1:400]\n",
    "    ans = zeros(3)\n",
    "    for i = 1:400\n",
    "        ans = ans + wi[i] * us[:,i]\n",
    "    end\n",
    "    return ans\n",
    "end\n",
    "\n",
    "function gradDescent(x0, step_size, tol)\n",
    "    H = zeros(3,3)\n",
    "    for i = 1:400\n",
    "       H = H + (us[:,i]) * (us[:,i])'\n",
    "    end\n",
    "    (D,) = eig(H)\n",
    "    L = maximum(D)/4\n",
    "    α = (1/L)\n",
    "    g0 = grad(x0)\n",
    "    its = 0\n",
    "    xnew = x0\n",
    "    for i in 1:step_size\n",
    "        xold = xnew\n",
    "        xnew = xold - ifelse(i<step_size/2, α*2, α*1.1) * grad(xold)\n",
    "        if norm(grad(xnew), 2) < norm(g0, 2)*tol \n",
    "            break\n",
    "        end\n",
    "        its = its + 1\n",
    "    end\n",
    "    return xnew, its\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the Newton methods and Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-4.949378062066131,0.0026906835956323786,0.7546868558708361],4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013838 seconds (172.93 k allocations: 9.125 MB)\n"
     ]
    }
   ],
   "source": [
    "# Simple Newton Method\n",
    "@time newtmin(logRegObj, [0,0,0], btFlag=false, BkFlag=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-4.94937806206612,0.002690683595632377,0.7546868558708332],4)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.077006 seconds (518.77 k allocations: 27.397 MB, 8.60% gc time)\n"
     ]
    }
   ],
   "source": [
    "# Newton Method with Modifed Hessian and Back tracking.\n",
    "@time newtmin(logRegObj, [0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-4.949377962777058,0.002690678758128348,0.7546875383267416],5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.097869 seconds (778.11 k allocations: 41.068 MB, 7.16% gc time)\n"
     ]
    }
   ],
   "source": [
    "# BFGS Quasi-Newton Method\n",
    "@time newtminBFGS(logRegObj, [0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-4.948489220582841,1.0761880719107262,0.7544687953680959],10000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".205801 seconds (248.15 M allocations: 12.703 GB, 10.54% gc time)\n"
     ]
    }
   ],
   "source": [
    "# Steepest Descent\n",
    "@time gradDescent([0,0,0], 10000, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the performance\n",
    "\n",
    "We can see that, all the Newton methods require only a few steps (4 or 5 steps) to satisfy the tolerace.\n",
    "However, the steepest descent require more than 10000 steps.\n",
    "Newton methods have much better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
